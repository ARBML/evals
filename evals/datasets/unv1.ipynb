{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gdown https://drive.google.com/uc?id=13GI1F1hvwpMUGBSa0QC6ov4eE57GC_Zx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xzvf UNv1.0.testsets.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = \"ar\"\n",
    "trg = \"en\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_msg = f\"Translate the following statement from {src} to {trg}\"\n",
    "def create_chat_prompt(sys_msg, input_text):\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": sys_msg}, \n",
    "        {\"role\": \"user\", \"content\": input_text}\n",
    "    ]\n",
    "\n",
    "def create_chat_example(article, summary):\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": article, \"name\": \"example_user\"},\n",
    "        {\"role\": \"system\", \"content\": summary, \"name\": \"example_assistant\"},\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ../registry/data/unv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "src_path = \"testsets/testset/UNv1.0.testset.ar\"\n",
    "trg_path = \"testsets/testset/UNv1.0.testset.en\"\n",
    "\n",
    "df_test_tar = pd.read_csv(trg_path, delimiter=\"  \", header = None)\n",
    "df_test_src = pd.read_csv(src_path, delimiter=\"  \", header = None)\n",
    "\n",
    "df_test = pd.concat([df_test_tar, df_test_src], axis = 1)\n",
    "df_test.columns = ['target', \"source\"]\n",
    "\n",
    "src_path = \"testsets/devset/UNv1.0.devset.ar\"\n",
    "trg_path = \"testsets/devset/UNv1.0.devset.en\"\n",
    "\n",
    "df_dev_tar = pd.read_csv(trg_path, delimiter=\"  \", header = None)\n",
    "df_dev_src = pd.read_csv(src_path, delimiter=\"  \", header = None)\n",
    "\n",
    "df_dev = pd.concat([df_dev_tar, df_dev_src], axis = 1)\n",
    "df_dev.columns = ['target', \"source\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dev[\"sample\"] = df_dev.apply(lambda x: create_chat_example(x['source'], x['target']), axis=1)\n",
    "df_dev[[\"sample\"]].to_json(f'../registry/data/unv1/few_shot_{src}_{trg}.jsonl', lines=True, orient=\"records\")\n",
    "\n",
    "df_test[\"input\"] = df_test['source'].apply(lambda x: create_chat_prompt(sys_msg, x))\n",
    "df_test[\"ideal\"] = df_test['target']\n",
    "df_test[[\"input\", \"ideal\"]].to_json(f'../registry/data/unv1/samples_{src}_{trg}.jsonl', lines=True, orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tagging_task_specs = \"\"\"\n",
    "unv1:\n",
    "    id: unv1.test.v1\n",
    "    metrics: [accuracy]\n",
    "    description: Evaluate Arabic translation\n",
    "# Define the eval\n",
    "unv1.test.v1:\n",
    "  # Specify the class name as a dotted path to the module and class\n",
    "  class: evals.elsuite.translate:Translate\n",
    "  args:\n",
    "    samples_jsonl: unv1/samples_ar_en.jsonl\n",
    "    num_few_shot: 0 # max few shots to use\n",
    "\n",
    "\"\"\".strip()\n",
    "with open(\"../registry/evals/unv1.yaml\", \"w\") as file:\n",
    "    file.write(pos_tagging_task_specs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "os.environ[\"EVALS_THREADS\"] = \"3\"\n",
    "os.environ[\"EVALS_THREAD_TIMEOUT\"] = \"600\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_path = \"../eval_results/unv1.jsonl\"\n",
    "!oaieval gpt-3.5-turbo-0301 unv1 --record_path {record_path} --seed 41 --modelspec_extra_options temperature=0.0 --max_samples 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "with open(record_path, \"r\") as f:\n",
    "    events_df = pd.read_json(f, lines=True)\n",
    "print(events_df[events_df[\"final_report\"].notnull()][\"final_report\"].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sacrebleu.metrics.bleu import BLEU\n",
    "\n",
    "for i, r in pd.json_normalize(events_df[events_df.type == \"sampling\"].sort_values('sample_id').data).iterrows():\n",
    "    print(f\"Prompt: {r.prompt[-1]}\")\n",
    "    print(f\"Sampled: {r.sampled}\")\n",
    "    print(f\"Truth: {df_test['target'][i]}\")\n",
    "    print(\"score\", BLEU(effective_order = True).sentence_score(r.sampled, [df_test['target'][i]]).score)\n",
    "    print(f\"{i}\",\"--\" * 25)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_df[events_df.type == \"sampling\"].sort_values('sample_id')['sample_id']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
